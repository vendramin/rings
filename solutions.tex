% \chapter*{Some solutions}

% \pagestyle{plain}
% \fancyhf{}
% \fancyhead[LE,RO]{Rings and modules}
% \fancyhead[RE,LO]{Some solutions}
% \fancyfoot[CE,CO]{\leftmark}
% \fancyfoot[LE,RO]{\thepage}

% \addcontentsline{toc}{chapter}{Some solutions}

\section*{Some solutions}

\fancyhf{}
\fancyfoot[R]{\thepage}
\fancyhead[L]{\course}
\fancyhead[R]{Some solutions}
\setlength{\headheight}{14pt}

\begin{sol}{xca:basic_formulas}\
\begin{enumerate}
    \item Since $x0=x(0+0)=x0+x0$, it follows that $x0=0$. Similarly, $0x=0$. 
    \item $0=x0=x(y+(-y))=xy+x(-y)$. Thus $-xy=x(-y)$.
    \item Assume that $1=0$ and let $x\in R$. Using the first item, 
        $x=x1=x0=0$.  
\end{enumerate}
\end{sol}

 \begin{sol}{xca:subrings}\
     \begin{enumerate}
         \item To see that $S\cap T$ is a subring, we first note that
             $S\cap T$ is a subgroup of $(R,+)$, as  
             the intersection of subgroups is a subgroup. Since $S$ and $T$ are subrings, $1\in S\cap T$. 
             Now let $x,y\in S\cap T$. Then 
             $xy\in S$ and $xy\in T$, because $S$ and $T$ 
             are subrings. Thus $xy\in S\cap T$. 
        \item To see that $\cup_{i\geq1}R_i$ is a subring, we first note that $\left(\cup_{i\geq1}R_i, +\right)$ is a subgroup of $(R,+)$  as the union of a sequence of subgroups is a subgroup.
        Since $R_i$ are subrings, $1\in R_i$ for every $i$, hence $1\in \cup_{i\geq1}R_i$.
        Finally let $x,y\in \cup_{i\geq1}R_i$, then there exist $i,j\geq 1$ such that $x\in R_i$ and $y\in R_j$. Let $k=\max(i,j)$, then $R_i\subseteq R_k$ and $R_j\subseteq R_k$. Therefore $x,y\in R_k$ and, since $R_k$ is a subring, $xy\in R_k\subseteq\cup_{i\geq1}R_i.$
        \item Suppose, by contradiction, that $S\cup T$ is 
        a subring of $R$ and that there exist $s\in S\setminus T$ and $t\in T\setminus S$.
        Then $s+t\in S\cup T$, so either $s+t\in S$ or $s+t\in T$. In both cases, we obtain a contradiction since either $t=(s+t)-s\in S$ or $s=(s+t)-t\in T $.
     \end{enumerate}
 \end{sol}

 \begin{sol}{xca:units_R[X]}
     We claim that $\mathcal{U}(\R[X])=\mathcal{U}(\R)=\R\setminus\{0\}$.
     Let $f\in\mathcal{U}(R[X])$. Then there exists $g\in \R[X]$ such that $fg=1$.
     In particular, since the degree of the product of two polynomials over a field is the sum of their degrees,
     $0=\deg(fg)=\deg(f)+\deg(g)$.
     Thus $\deg(f)=\deg(g)=0$ and hence $f$ is an invertible element of $\R$.
 \end{sol}

 \begin{sol}{xca:Qsqrt2}
     Let $x+y\sqrt{2}\in \Q[\sqrt{2}]\setminus\{0\}$.
     Observe that $(x+y\sqrt{2})(x-y\sqrt{2})=x^2-2y^2\in\Q.$
     Therefore 
     \[
     (x+y\sqrt{2})\left(\frac{x}{x^2-2y^2}-\frac{y\sqrt{2}}{x^2-2y^2}\right)=\frac{(x+y\sqrt{2})(x-y\sqrt{2})}{x^2-2y^2}=\frac{x^2-2y^2}{x^2-2y^2}=1
     \]
     and so $\frac{x}{x^2-2y^2}-\frac{y\sqrt{2}}{x^2-2y^2}$
 is the inverse of $x+y\sqrt{2}$ in $\Q[\sqrt{2}]$.
 \end{sol}

 \begin{sol}{xca:ideals}\
     \begin{enumerate}
         \item We first note that $(\cap_\alpha I_\alpha,+)$ is a subgroup of $(R,+)$, as 
         the intersection of subgroups is a subgroup.
             Let now $r\in R$ and $y\in \cap_\alpha I_\alpha$.
             Then for every $\alpha$ we have that $y\in I_\alpha$, hence, since $I_\alpha$ is an ideal, $ry,yr\in I_\alpha$ for every $\alpha$.
             Thus $ry\in \cap_\alpha I_\alpha$ and $yr\in \cap_\alpha I_\alpha$.
         \item    We first note that $(\cup_{i\geq 1} I_i,+)$ is a subgroup of $(R,+)$, as  
          the union of a sequence of subgroups is a subgroup.
          Let now $r\in R$ and $y\in \cup_{i\geq 1} I_i$.
          Then, there exists $i\geq 1$ such that $y\in I_i$
          and so $ry,yr\in I_i$, since $I_i$ is an ideal of $R$. Thus $ry,yr\in \cup_{i\geq 1} I_i$.
     \end{enumerate}
 \end{sol}

 \begin{sol}{xca:ideals_Z}
     Let $I$ be an ideal of $\Z$, if $I=\{0\}$, then we are done as $I=0\Z$.
     Suppose now $I\setminus\{0\}\neq \emptyset$, then there is at least an element $i\in I\cap \Z_{>0}$.
     So we can consider $n=\min(I\cap \Z_{>0})$ and we claim that $I=n\Z$.

     Since $n\in I$ then $nx\in I$ for every $x\in \Z$, hence $n\Z\subseteq I$.

     Vice versa, if $y\in I$ we can divide $y$ by $n$ obtaining $y=nq+r$ for some $q\in \Z$ and $r\in\{0,\dots, n-1\}$.
     Hence $r=y-nq\in I\cap \Z_{\geq 0}$ and $r<n$, so $r=0$ and thus $y=nq\in n\Z$ proving that $I\subseteq n\Z$.
 \end{sol}

 \begin{sol}{xca:ideals_Zn}
     We claim that the ideals of $\Z/n$ are of the form $d\Z/n$ for some $d\in\Z_{\geq 0}$ such that $d$ divides $n$.

     Let $I$ be an ideal of $\Z/n$, if $I=\{0\}$, then we are done as $I=0\Z/n$.
     Suppose now $I\setminus\{0\}\neq \emptyset$
     and let $d$ be the minimal in $\{1,\dots, n-1\}$ 
     such that $d\in I$.
     We claim that $I=n\Z$ and that $d$ divides $n$.

     If $d$ would not divide $n$,
     then $n=qd+r$ for some $q,r\in \Z$ with $0<r<d$.
     But then in $Z/n$ we would have $r=n-qd=-qd\in I$, a contradiction to the minimality of $d$.
      
     Since $d\in I$ then $dx\in I$ for every $x\in \Z/n$, hence $d\Z/n\subseteq I$.

     Vice versa, let $y\in I$, then $y=qd+r$ for some $q\in \Z$ and $r\in\{0,\dots, n-1\}$.
     Hence $r=y-qd\in I$ and $0\leq r<n$, so $r=0$ and thus $y=qd\in d\Z/n$ proving that $I\subseteq d\Z/n$.
 \end{sol} 

 \begin{sol}{xca:ideals_R}
     We claim that $\{0\}$ and $\R$ are the only ideals of $\R$.
     Suppose that $I$ is an ideal of $\R$ and that $I\neq\{0\}$.
     Then there exists $x\in I\setminus\{0\}$, but then $x$ is invertible and so $1=x^{-1}x\in \R I\subseteq I$.
     Therefore for every $r\in \R$ we have that $r=r1\in \R I\subseteq I$ and so $\R\subseteq I\subseteq \R$. Hence $I=\R$.
 \end{sol} 

 \begin{sol}{xca:ideals_matrices}
 We will denote with $E_{ij}$ the matrix in $M_n(R)$
 with all zeros and a 1 in position $(i,j)$.
 
Let $J$ be an ideal of $M_n(R)$. We claim that 
$E(J)=\{r\in R:rE_{11}\in J\}$ is an ideal of $R$ and that $J=M_n(E(J))$.

Using the fact that $(J,+)$ is a subgroup of $(R,+)$ we obtain that
$(E(J),+)$ is a subgroup of $R$.
More precisely, $0\in E(J)$ since $0E_{11}=0\in J$. Given $r,s\in E(J)$, i.e.
$rE_{11},sE_{11}\in J$, then $(r+s)E_{11}=rE_{11}+sE_{11}\in J$ and $(-r)E_{11}=-(rE_{11})\in J$, i.e. $r+s,-r\in E(J)$.

Moreover, since $J$ is an ideal of $R$,
$xrE_{11}=rxE_{11}=r(xE_{11})\in RJ\subseteq J$
for every $r\in R$ and $x\in E(J)$. 
So $E(J)$ is an ideal of $R$.

Finally, we can show that $J=M_n(E(J))$ and for that we are going to use the following property: given a matrix $A=(a_{ij})\in M_n(R)$,
\begin{equation}
\label{eq:elementary_matrices}
    E_{ki}AE_{jl}=a_{ij}E_{kl}\text{ for all }i,j,k,l.
\end{equation}


Consider $A=(a_{ij})\in J$,
then, by Equation \ref{eq:elementary_matrices},
$a_{ij}E_{11}=E_{1i}AE_{j1}\in J$ for every $i,j\in\{1,\dots, n\}$.
Hence $a_{ij}\in E(J)$ for every $i,j\in\{1,\dots, n\}$, i.e. $A\in M_n(E(J))$.
Thus $J\subseteq M_n(E(J))$.

Conversely, if $A=(a_{ij})\in M_n(E(J))$,
then $A=\sum_{i,j=1}^n a_{ij}E_{ij}$ with $a_{ij}\in E(J)$,
i.e. $a_{ij}E_{11}\in J$.
But using Equation \ref{eq:elementary_matrices} applied for $A=Id$
we also know that $E_{ki}E_{jl}=E_{kl}$, so
\[
a_{ij}E_{ij}=a_{ij}E_{i1}E_{1j}=a_{ij}E_{i1}E_{11}E_{1j}=E_{i1}(a_{ij}E_{11})E_{1j}\in J.
\]
Thus $A=\sum_{i,j=1}^n a_{ij}E_{ij}$ is a sum of elements of $J$, so $A\in J$.
Therefore $M_n(E(J))= J$.
 \end{sol}

\begin{sol}{xca:R[X]_principal}
Let $I$ be an ideal of $R[X]$. If $I=\{0\}$, then it is principal generated by $0$.

Suppose now $I\neq \{0\}$, then we can consider a polynomial $g(X)\in I$ 
such that $\deg(g(X))$ is minimal. We claim that $I=(g(X))$.

Clearly $(g(X))\subseteq I$, since $g(X)\in I$.

Conversely, consider $f(X)\in I$, then there exist 
$q(X),r(X)\in \R[X]$ with $r(X)=0$ or $\deg(r(X))<\deg(g(X))$
such that $f(X)=q(X)g(X)+r(X)$. 
Assume that $r(X)\neq 0$. Since 
\[
r(X)=f(X)-q(X)g(X)\in I,
\]
the minimality of $\deg g(X)$ implies that  $\deg(r(X))\geq\deg(g(X))$, a contradiction.
Thus $r(X)=0$ and hence $f(X)=q(X)g(X)\in (g(X))$. Therefore $I\subseteq (g(X))$.
\end{sol}
 
\begin{sol}{xca:x_unit}
If $x$ is a unit, then $yx=xy=1$ for some $y\in R$ and hence $ryx=r$ for all $r\in R$. Conversely, 
if $R=(x)$, then, in particular, $1\in R=(x)$ and hence $1=xy$ and $1=zx$ for some $y,z\in R$. Now
$z=z1=z(xy)=(zx)y=1y=y$ and hence $x$ is a unit. 
\end{sol}

\begin{sol}{xca:Z6->Z15}
    Let $f\colon\Z/6\to\Z/15$ be a ring homomorphism. Then 
    \[
    0=f(0)=f(6)=f(\underbrace{1+\cdots+1}_{6-\text{times}})=6f(1)=6,
    \]
    a contradiction, as $6\ne 0$ in $\Z/15$. 
\end{sol}

\begin{sol}{xca:evaluation_map}
    Let $x_0=f(X)\in \R$, and let $p(X)=\sum_{i=0}^n a_iX^i\in \R[X]$.
    Since $f$ is a ring homomorphism and $f(a_i)=a_i$ for every $i\in\{0,\dots, n\}$,
    \[
    f(p(X))
    =f\left(\sum_{i=0}^n a_iX^i\right)
    =\sum_{i=0}^n f(a_i)f(X)^i
    =\sum_{i=0}^n a_if(X)^i
    =\sum_{i=0}^n a_ix_0^i
    =p(x_0).
    \]
    So $f$ is the evaluation map at $x_0$
\end{sol}

\begin{sol}{xca:quotient_ring}
From group theory, we know that $R/I$ is abelian group with addition 
\[
    (x+I)+(y+I)=(x+y)+I
\]
and neutral element $0+I=I$. 

We have already seen that multiplication is well-defined. 
As an example, we also showed that 
the left distributive property holds, that is 
\[
 (x+I)\left((y+I)+(z+I)\right)=(x+I)(y+I)+(x+I)(z+I)
\]
for all $x,y,z\in R$. The right distributivity is similar: 
\[
 \left((x+I)+(y+I)\right)(z+I)=(x+I)(z+I)+(y+I)(z+I)
\]
for all $x,y,z\in R$. 
Let us prove the associativity of the
multiplication: 
\begin{align*}
    (x+I)\left((y+I)(z+I)\right) &= (x+I)(yz+I)\\
    &=x(yz)+I\\
    &=(xy)z+I\\
    &=(xy+I)(z+I)\\
    &=\left((x+I)(y+I)\right)(z+I).
\end{align*}
Finally, let us see why $1+I$ is the neutral element of the multiplication:
\[
(x+I)(1+I)=x1+I=x+I=1x+I=(1+I)(x+I)
\]
for all $x\in R$. 
\end{sol}

\begin{sol}{xca:first_iso}
    We first note that $K=\ker f$ is an ideal of $R$. Thus $R/K$ is a ring. 
    Let
    \[
    \varphi\colon R/K\to f(R),\quad x+K\mapsto f(x).
    \]
    
    We first need
    to prove that $\varphi$ is a well-defined map, that is
    if $x+K=y+K$, then $f(x)=f(y)$. 
    Thus we want to avoid situations as in the following picture: 
\begin{center}
\begin{tikzpicture}[scale=0.50]
\draw[thick] (7,0) ellipse[x radius=2, y radius=2.5]+(2,2.5) node{$f(R)$};
\draw[thick] (0,0) ellipse[x radius=3, y radius=2]+(2,2.25) node{$R/K$};
\coordinate (a) at (0,0);
\coordinate (z) at (6,1);
\coordinate (y) at (6.5,-1);
\fill (a) circle[radius=0.1] node[below]{$x+K=y+K$};
\fill (y) circle[radius=0.1] node[right]{$f(x)$};
\fill (z) circle[radius=0.1] node[right]{$f(y)$};
\draw[-Latex, shorten <=5pt, shorten >=5pt] (a) to[out=20, in=160] (y);
\draw[-Latex, shorten <=5pt, shorten >=5pt] (a) to[out=20, in=200] (z);
\end{tikzpicture}
\end{center}

    
    To prove this fact, we proceed as  
    follows:
    \begin{align*}
    x+K=y+K &\implies x-y\in K=\ker f\\
    &\implies f(x)-f(y)=f(x-y)=0\\
    &\implies f(x)=f(y).
    \end{align*}
    
    Now we need to prove that $\varphi$ is a ring homomorphism. First we
    prove that $\varphi(1+K)=1$:
    \[
    \varphi(1+K)=f(1)=1.
    \]
    To finish the proof of the fact that $\varphi$ is a ring homomorphism, 
    let $x,y\in R$. Then 
    \begin{align*}
        \varphi( (x+K)+(y+K)) &= \varphi(x+y+K)\\
        &=f(x+y)\\
        &=f(x)+f(y)\\
        &=\varphi(x+K)+\varphi(y+K).
    \shortintertext{and} 
        \varphi( (x+K)(y+K)) &= \varphi(xy+K)\\
        &=f(xy)\\
        &=f(x)f(y)\\
        &=\varphi(x+K)\varphi(y+K) 
    \end{align*}
    because $f$ is a ring homomorphism. 

    The map $\varphi$ is surjective: 
    if $y\in f(R)$, then $y=f(x)$ for some $x\in R$. Then 
    \[
    \varphi(x+K)=f(x)=y.
    \]

    Finally, we show that $\varphi$ is injective: 
    \begin{align*}
    \varphi(x+K)=\varphi(y+K) &\implies f(x)=f(y)\\
    &\implies x-y\in K=\ker f\\
    &\implies x+K=y+K.
    \end{align*}
\end{sol}

\begin{sol}{xca:sqrt2and3}
Suppose there exists an isomorphism $f\colon \Q[\sqrt{2}]\to\Q[\sqrt{3}]$. 
Since $f$ is additive and $f(1)=1$, $f(m)=m$ for all $m\in\Z$. Furthermore,
$f(m/n)=m/n$ for all $m/n\in\Q$. This implies that $f(a+b\sqrt{2})=a+bf(\sqrt{2})$. 
Let $z=f(\sqrt{2})$. Then 
\[
z^2=f(\sqrt{2})^2=f(2)=2
\]
and hence $z\in\{-\sqrt{2},\sqrt{2}\}$. But there are no numbers 
$\Q[\sqrt{3}]$ whose square is $2$: if \[
\left(a+b\sqrt{3}\right)^2=2
\]
for some
$a,b\in\Q$, then 
\[
2=\left(a+b\sqrt{3}\right)^2=a^2+3b^2+2ab\sqrt{3}.
\]
Since $\sqrt{3}\not\in\Q$, either $a=0$ or $b=0$. Both cases yield 
a contradiction. 
\end{sol}

\begin{sol}{xca:Z[i]/(1+3i)}
    Let $\varphi\colon\Z\to\Z[i]\xrightarrow{\pi}\Z[i]/(1+3i)$, where $\pi$ denotes the canonical map. 
    We claim that $\varphi$ is a surjective ring homomorphism with kernel $\ker\varphi=10\Z$. Then
    \[
    \Z/10\Z\simeq\varphi(\Z)=\Z[i]/(1+3i)
    \]
    by the first isomorphism theorem. 

    Note first that $\varphi$ is a ring homomorphism, as it is 
    the composition of ring homomorphisms. 
    
    We now show that $\ker\varphi=10\Z$: 
    \begin{align*} 
    m\in\ker\varphi&\Longleftrightarrow \varphi(m)+m\in (1+3i)\\
    &\Longleftrightarrow m=(1+3i)(x+iy)\text{ 
    for some $x,y\in\Z$}\\
    &\Longleftrightarrow m=10x\text{ for some $x\in\Z$}\\
    &\Longleftrightarrow m\in 10\Z.
    \end{align*}

    Finally, to see that $\varphi$ is surjective it is enough to 
    see that $i\in\varphi(\Z)$, as 
    then, for $a,b\in\Z$, one has 
    $\varphi(a+3b)\equiv a+ib\bmod (1+3i)$. 
    To show that $i\in\varphi(\Z)$
    note
    that \[
    \varphi(3)=3\equiv i\bmod (1+3i),
    \]
    as $3-i=-i(1+3i)$. 
\end{sol}



\begin{sol}{xca:Z15}
    Suppose there is an isomorphism $\varphi\colon Z[i]/I\to \Z/15$ for some 
    ideal $I$. In particular, 
    \[ 
    \varphi(-1+I)+\varphi(1+I)=\varphi(I)=0
    \]
    and hence, since $\varphi(1+I)=1$, it follows that $\varphi(-1+I)=-1$. Since 
    $i^2\equiv -1\bmod I$, one also has $\varphi(i^2+I)=-1$. But there are no
    elements $x\in\Z/15$ such that $x^2=-1$, a contradiction. 
\end{sol}

\begin{sol}{xca:IJ}
    Let $K=\{uv:u\in I,\,v\in J\}$. Then
    $X^2\in K$ and $Y^2\in K$ but $X^2+Y^2\not\in K$, so $K$ is not an ideal.
\end{sol}

\begin{sol}{xca:strongly_regular}
    Assume first that $I\cap J=IJ$ holds for all ideals $I$ and $J$. 
    Let $a\in R$. Since $Ra=RaR$ and
    $Ra$ is an ideal, 
    \[
    Ra=(Ra)(Ra)=(RaR)a=(Ra)a=Ra^2.
    \]
    In particular, $a\in Ra^2$ and 
    hence $a=xa^2$ for some $x\in R$. 
    
    Conversely, assume that $R$ is strongly regular. Since $I$ and $J$ are
    ideals, $I\cap J\supseteq IJ$. 
    Let $a\in I\cap J$. There exists $x\in R$ such that
    $a=xa^2$. In particular, 
    $a=(xa)a\in IJ$.
\end{sol}

\begin{sol}{xca:Hilbert_powerseries}
    We want to imitate the proof of Hilbert's Theorem, but the degree used for polynomials cannot be used for power series.
    So we define the \emph{order} of a power series 
    $f=\sum_{i\geq 0}a_iX^i$ the smallest $d\in \Z_{\geq 0}$ such that $a_d\neq 0$. We also define the order of 0 as 0.
    
    We must show that every ideal of $R[\![X]\!]$ is finitely generated.
    Assume that
	there is an ideal $I$ of $R[\![X]\!]$ that is not finitely generated. In particular, $I\ne\{0\}$.
	Let $f_1(X)\in I\setminus\{0\}$ be of minimal order $n_1$. 
	Since $I$ is not finitely generated, it follows that 
	$I\ne (f_1(X))$. Let $f_2(X)\in I\setminus (f_1(X))$ be
	of minimal order $n_2$.
    In particular, the minimality of the order of $f_1(X)$ implies that 
	$n_2\geq n_1$. 
	We continue with this procedure. For $i>1$ let 
	$f_i(X)\in I$ be a series of minimal order $n_i$ such that  
	such that $f_i(X)\not\in(f_1(X),\dots,f_{i-1}(X))$ (note
	that such an $f_i(X)$ exists because $I$ is not finitely generated). 
	Moreover, $n_i\geq n_{i-1}$. This happens because 
	if $n_i<n_{i-1}$, then
	$f_i(X)\not\in (f_1(X),\dots,f_{i-1}(X))$, which contradicts
	the minimality of $n_{i-1}=\deg f_{i-1}(X)$. 
	For each $i\geq1$ 
	let $a_i$ be the coefficient of $f_i(X)$ 
    related to its order, that is
	\[
	f_i(X)=a_iX^{n_i}+\cdots,
	\]
	where the dots denote a series of 
	order $>n_i$. Note that 
	$a_i\ne 0$ for all $i\geq 1$. 
	
	Let $J=(a_1,a_2,\dots)$. Since $R$ is noetherian, the sequence
	\[
	(a_1)\subseteq (a_1,a_2)\subseteq\cdots(a_1,a_2,\dots,a_k)\subseteq\cdots
	\]
	stabilizes, so we may assume that 
	$J=(a_1,\dots,a_m)$ for some $m\in\Z_{>0}$. 
	In particular, there exist $u_1,\dots,u_m\in R$ such that 
	\[
	a_{m+1}=\sum_{i=1}^m u_ia_i.
	\]
	Let 
	\[
	g(X)=\sum_{i=1}^mu_if_i(X)X^{n_{m+1}-n_i}\in (f_1(X),\dots,f_m(X))\subseteq I.
	\]
    The order of $g(X)$ is $n_{m+1}$ and the coefficient of $g(X)$ of degree $n_{m+1}$ is $\sum_{i=1}^mu_ia_i=a_{m+1}$.
    Thus the order of $g(X)-f_{m+1}(X)$ is $<n_{m+1}$. 
	
	Since $f_{m+1}(X)\not\in (f_1(X)\dots,f_m(X))$ and $g(X)\in (f_1(X)\dots,f_m(X))$, 
	\[
	g(X)-f_{m+1}(X)\not\in (f_1(X),\dots,f_m(X)),
	\]
	a contradiction to the minimality of the order of $f_{m+1}$.  
\end{sol}

\begin{sol}{xca:principal=>noetherian}
	Let $I_1\subsetneq I_2\subsetneq$ be a sequence of ideals of $R$.  
	Since $R$ is principal, each $I_j$ is principal, 
	say $I_j=(a_j)$ for some $a_j\in R$, so the sequence is of the form
	\[
	(a_1)\subsetneq (a_2)\subsetneq\cdots.
	\]
	Since $I=\cup_{i\geq1}(a_i)$ is an ideal of $R$, 
	there exists $x\in R$ such that $I=(x)$. Since $x\in (a_n)$ for some $n\in\Z_{>0}$, 
	it follows that $(a_k)\subseteq I=(x)\subseteq (a_n)$ for all $k\in\Z_{>0}$. 
\end{sol}

\begin{sol}{xca:content}
	Let $d$ be 
	the greatest common divisor
	of the coefficients of $f$. 
	Since $a$ divides all the coefficients of $f$, it follows that $a$ divides $d$. If $p$ is a prime that
	divides $d$, then either $p$ divides $a$ or $p$ divides all the coefficients of the primitive
	polynomial $f_1$. It follows that $d$ divides $a$.    
\end{sol}

\begin{sol}{xca:Gauss}\
\begin{enumerate}
	\item Let $f=\sum_{i=0}^na_iX^i$, $g=\sum_{i=0}^mb_iX^i$. Suppose that 
	$fg$ is not primitive and let $p$ be a prime number dividing all the coefficients  
	of $fg$. Since both $f$ and $g$ are primitive, there exist $i\in\{0,\dots,n\}$ 
 	and $j\in\{0,\dots,m\}$ minimal such that $p\nmid a_i$ and $p\nmid b_j$. 
 	If $c_{i+j}$ is the coefficient of $X^{i+j}$ in $fg$, then  
	\[
 	c_{i+j}=\sum_{k>i}a_kb_{i+j-k}+\sum_{k<i}a_kb_{i+j-k}+a_ib_j.
 	\]
 	Thus $p$ divides $\sum_{k>i}a_kb_{i+j-k}+\sum_{k<i}a_kb_{i+j-k}$ and does not divide
 	the integer $a_ib_j$, that is $p$ does not divide $c_{i+j}$, a contradiction.
	\item Suppose that $f$ is irreducible in $\Z[X]$. If $f$ is not primitive, then
	$f=af_1$ for some $a\in\Z\setminus\{-1,1\}$ and some primitive polynomial $f_1$, a contradiction 
	to the irreducibility of $f$. Let us prove that $f$ is irreducible in $\Q[X]$. If not, say 
	$f=gh$ for some $g,h\in\Q[X]$ of positive degree. After multiplying by a 
	suitable rational number, we may assume that  
	\[
 	f=\frac{a}{b}g_1h_1,
 	\]
 	where $\gcd(a,b)=1$ and $g_1,h_1\in\Z[X]$ are primitive polynomials, that is 
 	\[
 	bf=ag_1h_1.
 	\]
 	The greatest common divisor of the coefficients of $bf$ is $b$. 
	Since $g_1h_1$ is primitive by the previous item, the greatest common divisor 
	of the coefficients of $ag_1h_1$ is $a$. Thus $a=b$ or $a=-b$, 
  	that is $f=g_1h_1$ or $f=-g_1h_1$ in $\Z[X]$.  
\end{enumerate}
\end{sol}

\begin{sol}{xca:Z[X]_UFD}
	Since $\Z$ is noetherian, it follows that $\Z[X]$ is noetherian by Hilbert's theorem. 
	Then $\Z[X]$ admits factorizations. We need to show that the factorization into irreducibles is unique. 
	Let $f\in\Z[X]$ be non-zero and
	assume that 
	\[
	f=f_1\cdots f_k=g_1\cdots g_l
	\]
	be factorizations of $f$ into non-constant irreducibles integer polynomials. Since
	$f_1,\dots,f_k$ and $g_1,\dots,g_k$ 
	are irreducible in $\Q[X]$ and $\Q[X]$ is a unique factorization domain, it follows
	that $k=l$ and there exits $\sigma\in\Sym_k$ such that $g_i$ and $h_{\sigma(i)}$ are
	associate for all $i\in\{1,\dots,k\}$. After reordering we may assume that 
	for each $i\in\{1,\dots,k\}$ there
	exists $a_i/b_i\in\Q$ such that $b_ig_i=a_ih_i$. Since both $g_i$ and $h_i$ are
	irreducible integer polynomials, it follows from Exercise \ref{xca:Gauss} that
	both $g_i$ and $h_i$ are primitive. By Exercise \ref{xca:content}, 
	$a_i$ (resp. $b_i$) is the greatest common divisor of the coefficients 
	of $a_ih_i$ (resp. $b_ig_i$). This implies that $a_i$ and $b_i$ are associate, so 
	$a_i/b_i$ is a unit. Hence $g_i$ and $h_i$ are associate in $\Z[X]$.     
\end{sol}

%\begin{sol}{xca:Z[i]irreducibles}
%	Let $x\in\Z[i]$ be irreducible. Since 
%	$x\not\in\mathcal{U}(\Z[i])$, it follows that $N(x)>1$. Write
%	the integer $N(x)$ as a product of (not necessarily different) primes $p_1,\dots,p_k$. Since 
%	$x$ divides $N(x)=x\overline{x}=p_1\cdots p_k$ and $x$ is prime, 
%	$x$ divides $p_i$ for some $i\in\{1,\dots,k\}$. 
%	
%	If $p_i$ is irreducible in $\Z[i]$, 
%	then $x$ and $p_i$ are associate, so $x\in\{p_i,-p_i,ip_i,-ip_i\}$ for some
%	prime number $p_i$ that is irreducible in $\Z[i]$. 
%	
%	If $p_i$ is not irreducible, say $p_i=\alpha\beta$ for some $\alpha,\beta\in\Z[i]$ that
%	are not units of $\Z[i]$. Then 
%	$p_i^2=N(p_i)=N(\alpha)N(\beta)$ with $N(\alpha)>1$ and $N(\beta)>1$. The unique factorization
%	of $\Z$ implies that $N(\alpha)=N(\beta)=p_i$ and hence $\alpha$ and $\beta$ 
%	are irreducible in $\Z[i]$. Thus $x$ divides 
%	$\alpha\beta$. If $x$ divides the irreducible $\alpha$, 
%	then $x$ and $\alpha$ are associate in $\Z[i]$. This means that 
%	$\alpha\in\{x,-x,xi,-xi\}$ and $N(x)=N(\alpha)=p_i$ for some prime number $p_i$.  
%	
%	Combining the previous paragraphs with Fermat's theorem it follows that 
%	the irreducibles of $\Z[i]$ are $1+i$, $1-i$, $-1+i$ and $-1-i$, 
%	$p$, $-p$, $pi$ and $-p$ for some prime number $p\in\Z$ such that $p\equiv3\bmod 4$, and 
%	the elements $x\in\Z[i]$ such that $N(x)=p$ where $p\in\Z$ is a prime number such that $p\equiv1\bmod 4$. 
%\end{sol}

\begin{sol}{xca:p=1(4)}
    Since $p\equiv1\bmod4$, 
    $p$ is not prime in $\Z[i]$ (see Theorem \ref{thm:Fermat}). 
    Thus $p=\alpha\beta$ for some $\alpha,\beta\not\in\mathcal{U}(\Z[i])$. Hence $p^2=\operatorname{norm}(p)=\operatorname{norm}(\alpha)\operatorname{norm}(\beta)$. Since 
    $\operatorname{norm}(\alpha)\ne 1$ and $\operatorname{norm}(\beta)\ne 1$, 
    \[ 
    \operatorname{norm}(\alpha)=\operatorname{norm}(\beta)=p.
    \]
    Therefore $\alpha\overline{\alpha}=\operatorname{norm}(\alpha)=p$. 
\end{sol}

\begin{sol}{xca:Z[i]irreducibles}
    We claim that the set of irreducible
    elements of $\Z[i]$ 
    is the union of the following three subsets:
    \begin{align*}      
    &\{1+i,1-i,-1+i,-1-i\},\\
    &\{a+bi: a,b\in \Z, a^2+b^2=p \text{ is a prime
    such that }p\equiv1\bmod 4\},\\
    &\{p,-p,ip,-ip: p \text{ prime }p\equiv3\bmod 4\}
    \end{align*}
    
    Let $\alpha\in\Z[i]$ be irreducible. Then $\alpha$ is
    prime in $\Z[i]$. We claim that $p\in (\alpha)$ for some
    prime number $p\in\Z$. In fact, 
    $(\alpha)\cap\Z$ is an ideal of $\Z$, so 
    $(\alpha)\cap\Z=(p)$ for some positive integer $p\in\Z$. 
    To see that $p$ is a prime number, let 
    us assume that $p\mid xy$ for some $x,y\in\Z$. Then 
    $xy=pm\in (\alpha)\cap\Z$ for some $m\in\Z$. In particular, 
    $xy=\alpha\beta$ for some $\beta\in\Z[i]$. Thus $\alpha\mid xy$
    in $\Z[i]$ and therefore 
    $\alpha\mid x$ or $\alpha\mid y$ in $\Z[i]$. Thus 
    $x\in (\alpha)\cap\Z=(p)$ or $y\in(\alpha)\cap\Z=(p)$. Hence
    $p\mid x$ or $p\mid y$ in $\Z$, so $p$ is a prime number. 

    Let $\beta\in\Z[i]$ be such that $p=\alpha\beta$. 
    Then 
    \[
    p^2=\operatorname{norm}(p)=\operatorname{norm}(\alpha\beta)=\operatorname{norm}(\alpha)\operatorname{norm}(\beta).
    \]
    Thus $\operatorname{norm}(\alpha)\in\{p,p^2\}$, 
    as $\operatorname{norm}(\alpha)\ne1$ because $\alpha\not\in\mathcal{U}(\Z[i])$. 
    
    Assume first that $\operatorname{norm}(\alpha)=p$. By Lemma \ref{lem:norm},
    every such $\alpha$ is irreducible. It remains to check which prime numbers $p$ can be written as $\operatorname{norm}(\alpha)$.
    Write $\alpha=a+bi$ 
    for $a,b\in\Z$. So $p=\operatorname{norm}(\alpha)=a^2+b^2$ 
    and by Theorem \ref{thm:Fermat}, we have that $p=2$
    or $p\equiv1\bmod 4$.
    Moreover if $p=2$, then $a^2=b^2=1$, so $\alpha\in\{1+i,1-i,-1+i,-1-i\}$.
    
    Assume now that $\operatorname{norm}(\alpha)=p^2$. Then $\beta\in\mathcal{U}(\Z[i])$.
    So $p=\alpha\beta\in\{\alpha,-\alpha, i\alpha,-i\alpha\}$.
    Therefore $\alpha\in\{p,-p,ip,-ip\}$ and is irreducible. 
    Hence, by Theorem \ref{thm:Fermat}, $p\equiv3\bmod 4$.
\end{sol}   

%\section*{Lecture 5}

\begin{sol}{xca:maximal<=>field}
    Assume that $I$ is a maximal ideal of $R$. 
    We need to show that every non-zero element of $R/I$ 
    is invertible. Recall 
    that $1_{R/I}=1+I$. 
    Let $a\not\in I$. Then $(I,a)=R$, as
    $I$ is maximal. Then $1+x+ab$ for some $x\in I$ and $b\in R$.
    In particular, 
    \[
    (ab)+I=(a+I)(b+I)=1+I.
    \]
    
    Conversely, let $J$ be an ideal of $R$ 
    such that $I\subsetneq J$. Let $a\in J\setminus I$. 
    Since $R/I$ is a field, there exists $b\in R$ such that
    \[
    (ab)+I=(a+I)(b+I)=1+I.
    \]
    Thus $ab=1+x$ for some $x\in I\subseteq J$. Since $ab\in J$, 
    it follows that $1\in J$ and therefore $J=R$. 
\end{sol}

\begin{sol}{xca:prime<=>domain}
    Recall that $R/I$ is a ring with 
    $0_{R/I}=I$. Thus $x+I=0_{R/I}$ if and only if $x\in I$. 
    Assume first that $I$ is a prime ideal. 
    Let $a,b\in R$ be such that 
    \[
    (ab)+I=(a+I)(b+I)=I.
    \]
    Since
    $ab\in I$ and $I$ is prime, $a\in I$ or $b\in I$. This means
    that $a+I=I$ or $b+I=I$. 
    
    Conversely, if there are $a,b\in R$ 
    are such that $a\not\in I$ and $b\not\in I$ and 
    \[
    (ab)+I=(a+I)(b+I)=I,
    \]
    then $ab\in I$ and $I$ is not prime. 
\end{sol}

\begin{sol}{xca:maximal=>prime}\
\begin{enumerate}
    \item Let $a,b\in R$ be such that $ab\in I$. If $a\not\in I$, 
    then $(I,a)=R$, as $I$ is a maximal ideal. Thus $1=x+ra$ for
    some $x\in I$ and $r\in R$. Multiplying by $b$ and using
    that $x\in I$ and $ab\in I$,  
    $b=xb+rab\in I$. 
    \item The ideal $(X)$ is prime as $R/(X)\simeq\Z$ is a domain. Since     $\Z$ is not a field, the ideal $(X)$ cannot be maximal (see
        Exercise \ref{xca:maximal<=>field}). 
    \item Let $J$ be an ideal such that $I\subseteq J$. There 
        exist $a,b\in R\setminus\{0\}$
        such that $I=(a)$ and $J=(b)$. In particular, 
        since $a\in I\subseteq J$, there exists $r\in R$ such that
        $a=rb\in I$. Since $I$ is prime, $r\in I$ or $b\in I$. 
        If $b\in I$, then $I=J$. If $r\in I=(a)$, then 
        $r=sa$ for some $s\in R$. Thus  
        $a=rb=s(ab)$, so $a(1-sb)=0$. Since $a\ne 0$, 
        $b\in\mathcal{U}(R)$ and therefore $J=R$. 
\end{enumerate}
. 
\end{sol}

\begin{sol}{xca:fx=cx}
    Consider $\R$ as a $\Q$-vector space. 
    Since $\sqrt{2}\not\in\Q$, the subset $\{1,\sqrt{2}\}$ of $\R$ 
    is linearly independent 
    over $\Q$. Extend $\{1,\sqrt{2}\}$ 
    to a basis $B$ of $\R$ as a $\Q$-vector space. The
    linear map $f\colon\R\to\R$ such that
    $f(1)=\sqrt{2}$, $f(\sqrt{2})=1$ and $f(b)=b$ for all $b\in B\setminus\{1,\sqrt{2}\}$ 
    is not of the form $f(x)=\lambda x$. Indeed, if $f(x)=\lambda x$ for some $\lambda\in\R$, 
    then $\sqrt{2}=f(1)=\lambda$ and $1=f(\sqrt{2})=\lambda\sqrt{2}$, a contradiction. 
\end{sol}

\begin{sol}{xca:Rn=R}
    Let $\{x_i:i\in I\}$ be a basis of $\R$ as a $\Q$-vector space and
    let $\{e_1,\dots,e_n\}$ be the standard basis of $\R^n$ as a vector space over $\R$. 
    Routine calculations prove that  
    \[
    \{x_ie_j:i\in I,1\leq j\leq n\}
    \]
    is a basis of $\R^n$ as a $\Q$-vector space. 
    Since
    $I$ and $I\times\{e_1,\dots,e_n\}$ have the same cardinality, 
    there exists a bijective map $f\colon I\to I\times\{e_1,\dots,e_n\}$. This bijective
    map extends to a linear isomorphisms between the rational vector spaces 
    $\R^n$ and
    $\R$. In particular, $\R^n$ and 
    $\R$ are isomorphic as abelian groups. 
\end{sol}


%\section*{Lecture 7}



%\section*{Lecture 6}


% \begin{sol}{xca:projector}
% \end{sol}

% \begin{sol}{xca:submodules}
% \end{sol}

% \begin{sol}{xca:commuting}
% \end{sol}

% \begin{sol}{xca:Hom}
% \end{sol}
\begin{sol}{xca:aut}
    Let $G$ be a group such that $\Aut(G)=\{\id\}$. 
    Then $G$ is abelian, as the map $G\to G$, $x\mapsto gxg^{-1}$,   
    is an automorphism of $G$. Since $G$ is abelian, 
    the map $G\to G$, $x\mapsto x^{-1}$, is an automorphism of $G$. Since 
    the automorphism group of $G$ is trivial, 
    $x=x^2$ for all $x\in G$. 
    At this point it is convenient to write $G$ additively. We claim that 
    $G$ is a vector space
    over the field $\Z/2$ of two elements, the action
    is $\Z/2\times G\to G$, $(\lambda,g)\mapsto \lambda g$. Note that
    we need $g+g=0$ to prove that $G$ is indeed a vector space over $\Z/2$, as for example
    \[
    0=0g=(1+1)g=1g+1g=g+g
    \]
    for all $g\in G$. 
    Let $B$ be   
    a basis of $G$ over $\Z/2$. If $|I|\geq 2$, 
    the automorphism of $G$ that exchanges to basis elements of $B$ and  
    and fixes all other elements of $B$ would be non-trivial. Hence $|I|=1$ and 
    $G$ is either trivial of cyclic of order two. Both groups have trivial
    automorphism group. 
\end{sol}


\begin{sol}{xca:freshman_dream}
    One approach to solve this exercise requires the following fact: 
    $p$ is a prime number and $1\leq k\leq p^n-1$, then $\binom{p^n}{k}$ is divisible by $p$.
    Let us prove this claim. Write
    \[
    \binom{p^n}{k}=\frac{(p^n)!}{k!(p^n-k)!}=\frac{p^n}{k}\binom{p^n-1}{k-1}.
    \]
    Then
    \[
    k\binom{p^n}{k}=p^n\binom{p^n-1}{k-1},
    \]
    that is $p^n$ divides $k\binom{p^n}{k}$. 
    
    If $\gcd(p,k)=1$, then it follows that $p$ divides $\binom{p^n}{k}$ by unique decomposition
    of every integer as a product of primes. We may assume then that $\gcd(p,k)\ne1$, 
    say $k=p^\alpha m$ for some integer $m$ not divisible by $p$. Then
    \[
    p^n\binom{p^n-1}{k-1}=k\binom{p^n}{k}=p^{\alpha}m\binom{p^n}{k}
    \]
    and hence 
    \[
    p^{n-\alpha}\binom{p^n-1}{k-1}=m\binom{p^n}{k}.
    \]
    Since $k<p^n$, it follows that
    $n-\alpha\geq 1$. Thus $p$ divides $m\binom{p^n}{k}$ and hence
    $p$ divides $\binom{p^n}{k}$ because $p$ and $m$ are coprime. 
\end{sol}


\begin{sol}{xca:RC3}
Let $G=\langle g:g^3=1\rangle$ be the cyclic group of order three and $\omega$ be a primitive cubic root of one.  
The map $G\to \R\times\C$, $g\mapsto (1,\omega)$, extends to an algebra
homomorphism $\varphi\colon\R[G]\to\R\times\C$. Since 
$\dim_\R\R\times\C=\dim\R[G]=3$, it follows that $\varphi$ is a bijective. 
\end{sol}


%\section*{Lecture 9}
\begin{sol}{xca:equivalence}
Let $T\colon V\to W$ be a bijective linear map such that $T\rho_g=\psi_gT$ for all $g\in G$. 
\begin{enumerate}
	\item Assume that $\psi$ is not irreducible. Let $Y$ be an invariant subspace of $W$. 
		Then $X=T^{-1}(Y)$ is an invariant subspace of $V$, as 
		\[
		\rho_g(X)=\rho_gT^{-1}(Y)=T^{-1}\psi_g(Y)\subseteq T^{-1}(Y)=X
		\]
		for all $g\in G$. 
	\item By assumption
		$V$ can be decomposed as $V=X\oplus Y$ for some invariant subspaces $X$ and $Y$. Let $X_1=T(X)$ 
		and $Y_1=T(Y)$. 
		Note that $X_1$ and $Y_1$ are invariant. In fact, if $g\in G$, then  
		$\psi_g(X_1)=\psi_g T(X)=T\rho_x(X)\subseteq T(X)=X_1$.
		Similarly, $\psi_g(Y_1)\subseteq Y_1$ 
		for all $g\in G$. Now we prove that $W=X_1\oplus Y_1$. If $w\in W=T(V)$, then 
		$w=T(v)$ for some $v\in V$ since $T$ is surjective. Write $v=x+y$ for $x\in X$ and $y\in Y$. Then 
		$w=T(v)=T(x)+T(y)\in X_1+Y_1$. Now if $w\in X_1\cap Y_1$, then $w=T(x)=T(y)$ for some $x\in X$ and
		$y\in Y$. Since $T$ is injective, it follows that $x=y\in X\cap Y=\{0\}$. Hence $w=0$. 
	\end{enumerate}	
\end{sol}

\begin{sol}{xca:not_decomposable}
    Let $\rho\colon \Z\to\GL_2(\C)$ be the homomorphism given by 
    $m\mapsto\begin{pmatrix}1&m\\0&1\end{pmatrix}$. Then $\rho$ is not irreducible, as
    since $v_1=\begin{pmatrix}1\\0\end{pmatrix}$
    an eigenvector of $\rho_m$ for all $m\in\Z$, 
    the subspace $\C v_1$ is an invariant subspace of $\C^2$. To prove that $\rho$ is 
    indecomposable, let us assume that 
    it is not, that is $\rho$ is equivalent to a direct sum of degree-one (and hence diagonal) 
    representations. However, $\rho_1=\begin{pmatrix}1&1\\0&1\end{pmatrix}$ is not diagonalizable, 
    a contradiction. 
\end{sol}



\begin{sol}{xca:Z4overZ2}
	Note that if $M$ is a module over $\Z/2$, then $2m=0$ 
	for all $m\in M$.  If
	$\Z/4$ is a moduel over $\Z/2$, 
	then $0=2\cdot 1=1+1=2$, a contradiction. 
\end{sol}

\begin{sol}{xca:submodules}\
    \begin{enumerate}
        \item We prove that $\{0\}$, $M$, $\R\times\{0\}$ and $\{0\}\times\R$ are
        the only submodules of $M$. If $N$ is a non-zero submodule of $M$, then
        let $(x_0,y_0)\in N\setminus\{(0,0)\}$. If $(x,y)\in M$ is such that $xy\ne 0$, then
        \[
            \left(\frac{x}{x_0}+\left(\frac{y}{y_0}-\frac{x}{x_0}\right)X\right)\cdot (x_0,y_0)=(x,y) 
        \]
        and thus $N=M$. If $y_0=0$, then $N=\R\times\{0\}$, as $\frac{x}{x_0}\cdot (x_0,0)=(x,0)$. If $x_0=0$, then
        $N=\{0\}\times\R$, as 
        $\frac{y}{y_0}\cdot (0,y_0)=(0,y)$ 
            \item If $N\subseteq M$ is a submodule, then $N$ is real vector space. Assume that 
            $N\ne\{(0,0)\}$ and that $N\ne\R^2$. Since $\dim N=1$, 
            let $\{(a_0,b_0)\}$ be a basis of $N$. Since $N$ is a submodule of $M$, 
            $(b_0,a_0)=X\cdot (a_0,b_0)\in N$. In particular, there exists 
            $\lambda\in\R$ such that $(b_0,a_0)=\lambda (a_0,b_0)$. Since $(a_0,b_0)\ne(0,0)$, 
            without loss of generality we may assume that 
            $a_0\ne 0$. Thus 
            $\lambda^2 a_0=\lambda (\lambda a_0)=\lambda b_0=a_0$ and hence
            $\lambda^2=1$. If $\lambda=1$, then 
            $a_0=b_0$. If $\lambda=-1$, then $a_0=-b_0$. In conclusion, $N$ 
            is generated either by $(1,1)$ or $(1,-1)$.  
    \end{enumerate}
\end{sol}


\begin{sol}{xca:Pruffer}
	Take for example the $\Z$-module $\bigcup_{k\geq1}\Z/(p^k)$.
\end{sol}

% \begin{sol}{xca:linear_algebra}
% Para demostrar la primera afirmación procederemos por inducción en la cantidad de generadores de $M$. Si $M=(m)$, entonces
% $\{m\}$ es base pues $\{m\}$ es linealmente independiente: si $r\cdot m=0$ y $r\ne 0$, entonces
% \[
% m=1\cdot m=(r^{-1}r)\cdot m=r^{-1}\cdot (r\cdot m)=0.
% \]
% Si vale para $k-1$ generadores, sea $M=(m_1,\dots,m_k)$. Si $\{m_1,\dots,m_k\}$ no es linealmente
% independiente, entonces existen $r_1,\dots,r_k\in R$ no todos cero tales
% que
% \[
% r_1\cdot m_1+\cdots+r_k\cdot m_k=0.
% \]
% Sin perder generalidad podemos suponer que $r_k\ne 0$. Entonces
% \[
% v_k=\sum_{i=1}^{k-1} (r_k^{-1}r_i)\cdot m_i\in (m_1,\dots,m_{k-1}).
% \]
% Como entonces $M=(m_1,\dots,m_k)=(m_1,\dots,m_{k-1})$, la hipótesis inductiva implica que
% $M$ es libre. 

% Vamos a demostrar ahora que todo
% conjunto $X$ linealmente independiente puede extenderse a una base. 	Sea $X=\{x_1,\dots,x_k\}$ tal que $M=(X)$. 
% Como $M\ne\{0\}$, sin perder generalidad podemos suponer que $x_1\ne 0$. Como $R$ es de división,
% el conjunto $\{x_1\}$ es linealmente independiente, pues si $r\ne 0$ y $r\cdot x_1=0$, entonces 
% \[
% x_1=1\cdot r=(r^{-1}r)\cdot x_1=r^{-1}\cdot (r\cdot x)=r^{-1}\cdot 0=0.
% \]
% Sea $Y=\{y_1,\dots,y_l\}$ un subconjunto de $X$ maximal tal que $Y$ es linealmente independiente. Veamos que $X\subseteq (Y)$. Sea $x\in X$. Si $x\not\in Y$,  
% entonces, como $Y\subseteq Y\cup \{x\}$, la maximalidad de $Y$ implica que $\{x\}\cup Y$ es linealmente dependiente, es decir
% que existen $r,r_1,\dots,r_k\in R$ no todos cero tales que
% \[
% r\cdot x+\sum_{i=1}^l r_i\cdot y_i=0.
% \]
% Si $r=0$, entonces $r_1=\cdots=r_l=0$ porque los $y_j$ son linealmente independientes, una contradicción. Luego $r\ne 0$ y entonces
% \[
% x=-\sum_{i=1}^l (-r^{-1}r_i)\cdot y_i\in (Y).
% \]
% Luego $X\subseteq (Y)$. En conclusión $Y$ es una base de $M$ pues $M=(Y)$ 
% y además $Y$ es linealmente independiente. 

% Demostremos que dos bases finitas cualesquiera tienen la misma cantidad de elementos. 
% Para eso es suficiente demostrar
% que si $X$ e $Y$ son conjuntos finitos linealmente independientes
% tales que $(X)\subseteq (Y)$, entonces $|X|\leq |Y|$. Supongamos que $|X|=k$ e $|Y|=l$. 
% Procederemos por inducción en $l$. Si $l=1$ y $k>1$, entonces
% exiten $r_1,r_2\in R$ tales que $x_1=r_1\cdot y_1$ y $x_2=r_2\cdot y_1$. Luego
% \[
% x_2=r_2\cdot y_1=r_2\cdot (r_1^{-1}\cdot x_1)=(r_2r_1^{-1})\cdot x_1,
% \]
% una contradicción pues $\{x_1,x_2\}$ es linealmente independiente. 
% Supongamos ahora que el resultado 
% es verdadero para $l-1$ y sea $l=|Y|$. Para cada $j$ escribimos
% \[
% x_j = \sum_{i=1}^l r_{ji}\cdot y_i,
% \]
% donde $r_{ji}\in R$. Si $r_{j1}\ne 0$ para todo $j$, entonces 
% $x_j=\sum_{i=2}^l r_{ji}\cdot y_i$ para todo $j$ y luego $(X)\subseteq (y_2,\dots,y_l)$, que implica que
% $|X|\leq l-1<l=|Y|$. Si existe $j$ tal que $r_{j1}\ne 0$, sin perder generalidad podemos
% suponer que $r_{11}\ne 0$. Para cada $j\in\{2,\dots,k\}$ sea
% \[
% z_j = x_j-(r_{j1}r_{11}^{-1})\cdot x_1.
% \]
% Como $z_j\in (y_2,\dots,y_l)$ para todo $j$ y los $z_j$ son linealmente independientes, 
% la hipótesis inductiva impica que $k-1\leq l-1$, es decir $|X|\leq |Y|$.  
% \end{sol}

%\begin{sol}{xca:directa_noetherian}
%	Use induction on $n$. This cannot extend to infinitely many
%	modules, take for example sequences of integers with finite support, 
%	that is maps $\Z_{>0}\to \Z$ with finitely many non-zero images.  
%\end{sol}

% \begin{sol}{xca:cardinality}
%     Let $E$ and $B$ be bases of $M$. Each $e\in E$ can be written as a finite linear combination
%     of elements of $B$. This means that for each $e\in E$ there is a finite subset $B(e)$ of $B$, 
%     so there is a map $e\mapsto B(e)$. Note that 
%     \[
%     \left|\bigcup_{e\in E} B(e)\right|\leq |E|.
%     \]
%     We claim that $\bigcup_{e\in E}B(e)=B$. Clearly  $\bigcup_{e\in E}B(e)\subseteq B$. 
%     To prove the other inclusion let $X=\bigcup_{e\in E}B(e)$. If $e\in E$, then 
%     $e$ is a finite linear combination of elements of $X$. In particular, 
%     every element of $B\setminus X$ is a finite linear combination of elements of $X$. 
    
%     let $B(e)$ be the 
% \end{sol}

\begin{sol}{xca:AX=b}
	The Smith's normal form yields 
	\[
		P=\begin{pmatrix}
			0 & 1 & 0\\
			1 & 0 & 0\\
			-2 & -2 & 1
		\end{pmatrix},
		\quad
		Q=\begin{pmatrix}
			1 & 1 & 1\\
			0 & 1 & 1\\
			0 & 0 & 1
		\end{pmatrix},\quad
		S=PAQ=\begin{pmatrix}
			1 & 0 & 0\\
			0 & 1 & 0\\
			0 & 0 & 1
		\end{pmatrix}.
	\]
	The solution of the integer linear system is then
	$X=\begin{pmatrix}
		1\\
		0\\
		5
	\end{pmatrix}$. 
\end{sol}

\begin{sol}{xca:factors_260}
	We do not provide the full solutions. It should be enough to
	show that the Smith's normal form
	of the matrix $\begin{pmatrix}
		6 & 6 & 4\\
		6 & 12 & 8\\
		0 & 0 & 0
	\end{pmatrix}$ is $\begin{pmatrix}
		2 & 0 & 0\\
		0 & 6 & 0\\
		0 & 0 & 0
	\end{pmatrix}$. 
\end{sol}

\begin{sol}{xca:C4}
    Let $A=\begin{pmatrix}
    3 & 2 & 1 \\
    8 & 4 & 2 \\
    7 & 6 & 2\\
    9 & 6 & 1
    \end{pmatrix}$.  
    The Smith's normal form of $A$  
    is $\begin{pmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0 \\
        0 & 0 & 4 \\
        0 & 0 & 0
    \end{pmatrix}$. Thus the abelian group is isomorphic to $\Z/\Z\times\Z/\Z\times\Z/4\Z\simeq\Z/4$.  
\end{sol}

 \begin{sol}{xca:Smith_Z[i]}
 	The Smith's normal form is $\begin{pmatrix}
 		1 & 0\\
 		0 & -11+8i
 	\end{pmatrix}$. 
\end{sol}

% K := QuadraticField(-1);
% Z<i> := RingOfIntegers(K); 
% m := Matrix([[1+i, 2-i],[3,5*i]]);
% SmithForm(m);
% [       1        0]
% [       0 11*i + 8]

% [   1    0]
% [-8*i    1]

% [      i 2*i + 1]
% [      1  -i + 1]

\begin{sol}{xca:Smith_Q[X]}
 	The Smith normal form is 
 	\[
 		\begin{pmatrix}
 			1 & 0 & 0 & 0 \\
 			0 & 1 & 0 & 0 \\
 			0 & 0 & 1 & 0 \\
 			0 & 0 & 0 & f(X)
 		\end{pmatrix},
 	\]
 	where $f(X)=X^4 - 3X^3 - 18X^2 + 56X$. 
%     Here is the code: 
% \begin{lstlisting}
%  R<x> := PolynomialRing(Rationals());
%  m := Matrix([[7, x, 0, -x], [0, x-3, 0, 3], [0, 0, x-4, 0], [x-6, -1, 0, x+1]]);
%  P<x> := PolynomialAlgebra(Rationals());
%  A := Matrix(P, 4, 4, [7, x, 0, -x,\ 
%  > 0, x-3, 0, 3,\ 
%  > 0, 0, x-4, 0,\
%  > x-6, -1, 0, x+1]);
%  ElementaryDivisors(A);
%  [
%      1,
%      1,
%      1,
%      x^4 - 3*x^3 - 18*x^2 + 56*x
%  ]
%  \end{lstlisting}
 \end{sol}

\begin{sol}{xca:unimodular}
    Since $\det A\ne 0$, the set $\{y_1,\dots,y_n\}$ is linearly independent over $\Q$
    and hence they are linearly independent over $\Z$. 
    Let $B=(b_{ij})\in M_n(\Q)$ be the inverse of $A$. Then 
    \[
    B=A^{-1}=(\det A)^{-1}\operatorname{adj}(A)=\pm\operatorname{adj}(A)\in M_n(\Z).
    \]
    Then $x_i=\sum_{j=1}^n b_{ij}y_j$ for all $i\in\{1,\dots,n\}$ 
    and hence $\{y_1,\dots,y_n\}$ generates $G$ and therefore 
    $\{y_1,\dots,y_n\}$ is a basis of $G$. 
    
    Assume now that $\{y_1,\dots,y_n\}$ is a basis of $G$. There exist matrices 
    $A=(a_{ij})\in M_n(\Z)$ and $B=(b_{ij})\in M_n(\Z)$ such that
    \[
    y_i=\sum_{j=1}^{n}a_{ij}x_j,\quad
    x_i=\sum_{j=1}^{n}b_{ij}y_j
    \]
    for all $i\in\{1,\dots,n\}$. In particular, 
    $AB=I$. Applying determinant, $(\det A)(\det B)=1$ and hence $\det A=\det B\in\{-1,1\}$. 
\end{sol}

% \begin{sol}{xca:Z3}
    
% \end{sol}
